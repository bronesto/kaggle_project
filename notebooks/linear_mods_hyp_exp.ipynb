{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.features import create_train_features\n",
    "from exp.run import run_experiment\n",
    "from exp.mappings import alg_map\n",
    "from exp.train import train_model\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_save = \"X_tr.csv\"\n",
    "y_save = \"y_tr.csv\"\n",
    "X_save_scaled = \"X_tr_scaled.csv\"\n",
    "scale_params_pickle = \"scale_params.pickle\"\n",
    "other_params_json = \"other.json\"\n",
    "tr_scaler = None\n",
    "classic_sta_lta5_mean_fill = None\n",
    "classic_sta_lta7_mean_fill = None\n",
    "\n",
    "if not (os.path.exists(X_save_scaled) and os.path.exists(y_save)):\n",
    "    if os.path.exists(X_save) and os.path.exists(y_save):\n",
    "        X_tr = pd.read_csv(X_save, index_col=0)\n",
    "        y_tr = pd.read_csv(y_save, index_col=0)\n",
    "\n",
    "        scale_params_pickle_on = open(scale_params_pickle, \"rb\")\n",
    "        tr_scaler = pickle.load(scale_params_pickle_on)\n",
    "        scale_params_pickle_on.close()\n",
    "        \n",
    "        X_train_scaled = pd.DataFrame(tr_scaler.transform(X_tr), columns=X_tr.columns)\n",
    "        X_train_scaled.to_csv(X_save_scaled)\n",
    "    else:\n",
    "        X_tr, X_train_scaled, y_tr, tr_scaler, classic_sta_lta5_mean_fill, classic_sta_lta7_mean_fill  = create_train_features(r'C:\\Users\\arvin\\dev\\lanl\\train.csv')\n",
    "        X_tr.to_csv(X_save)\n",
    "        y_tr.to_csv(y_save)\n",
    "        X_train_scaled.to_csv(X_save_scaled)\n",
    "\n",
    "        scale_params_pickle_on = open(scale_params_pickle, \"wb\")\n",
    "        pickle.dump(tr_scaler, scale_params_pickle_on)\n",
    "        scale_params_pickle_on.close()\n",
    "\n",
    "        with open(other_params_json, 'w') as fp:\n",
    "            json.dump({\"classic_sta_lta5_mean_fill\": classic_sta_lta5_mean_fill,\n",
    "                       \"classic_sta_lta7_mean_fill\": classic_sta_lta7_mean_fill}, fp)\n",
    "else:\n",
    "    X_train_scaled = pd.read_csv(X_save_scaled, index_col=0)\n",
    "    y_tr = pd.read_csv(y_save, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper-parameter experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of Cartesian Product of Hyper-parameters for Linear Regression\n",
    "\n",
    "\"lr\": {\"fit_intercept\": [False, True], \"normalize\": [False, True]}\n",
    "\n",
    "Cartesian Product: {fit_intercept} x {normalize}\n",
    "\n",
    "Hyper-parameter choices:\n",
    "\"fit_intercept\": False, \"normalize\": False\n",
    "\"fit_intercept\": True, \"normalize\": False\n",
    "\"fit_intercept\": False, \"normalize\": True\n",
    "\"fit_intercept\": True, \"normalize\": True\n",
    "\"\"\"\n",
    "\n",
    "params={\"lr\": {\"fit_intercept\": [False, True], \"normalize\": [False, True]},\n",
    "       \"ridge\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                 \"fit_intercept\": [False, True], \"normalize\": [False, True]},\n",
    "       \"lasso\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                 \"fit_intercept\": [False, True], \"normalize\": [False, True],\n",
    "                 \"positive\": [False, False, False, False, False, True],\n",
    "                 \"selection\": [\"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"random\"]},\n",
    "       \"mtlasso\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                   \"fit_intercept\": [False, True], \"normalize\": [False, True],\n",
    "                   \"selection\": [\"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"random\"]},\n",
    "       \"elastic\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                   \"fit_intercept\": [False, True], \"normalize\": [False, True], \n",
    "                   \"positive\": [False, False, False, False, False, True],\n",
    "                   \"l1_ratio\": [.01, .99, .2, .4, .6, .8], \n",
    "                   \"selection\": [\"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"random\"]},\n",
    "       \"lars\": {\"fit_intercept\": [False, True], \"normalize\": [False, True],\n",
    "                \"fit_path\": [False], \"n_nonzero_coefs\": [10, 100, 500, 1000, 10000, np.inf]},\n",
    "       \"llars\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                 \"fit_intercept\": [False, True], \"normalize\": [False, True],\n",
    "                \"fit_path\": [False], \"positive\": [False, False, False, False, False, True]},\n",
    "       \"omp\": {\"fit_intercept\": [False, True], \"normalize\": [False, True],\n",
    "               \"n_nonzero_coefs\": [10, 100, None, None, None]},\n",
    "       \"sgdreg\": {\"loss\": [\"squared_loss\", \"squared_loss\", \"squared_loss\", \"huber\", \"epsilon_insensitive\",\n",
    "                           \"squared_epsilon_insensitive\"],\n",
    "                  \"penalty\": [\"none\", \"l2\", \"l1\", \"elasticnet\"], \n",
    "                  \"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                  \"l1_ratio\": [.01, .99, .2, .4, .6, .8], \"fit_intercept\": [False, True],\n",
    "                  \"learning_rate\": [\"constant\", \"optimal\", \"optimal\", \"optimal\", \"invscaling\", \"adaptive\"],\n",
    "                  \"eta0\": [1.0, 10.0, .1, .01, .001, .0001],\n",
    "                  \"early_stopping\": [False, False, False, False, True]},\n",
    "       \"pareg\": {\"C\": [.001, .01, .1, 1.0, 1.0, 1.0, 10.0, 100.0],\n",
    "                 \"loss\": [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\n",
    "                 \"epsilon\": [.01, .05, .1, .1, .1, .5],\n",
    "                 \"early_stopping\": [False, False, False, False, True]},\n",
    "        # \"tsreg\": {\"fit_intercept\": [False, True]},\n",
    "        \"hreg\": {\"epsilon\": [1.1, 1.2, 1.35, 1.35, 1.35, 1.35, 1.5, 1.6, 1.8, 2.0, 2.5],\n",
    "                 \"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                 \"fit_intercept\": [False, True]},\n",
    "        \"kreg\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                 \"kernel\": [\"linear\", \"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "                 \"gamma\": [None, None, None, None, .001, .0001, .01, .1]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_searches=20\n",
    "n_fold=10\n",
    "save_results= \"exp.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alg in params.keys():\n",
    "    print(alg)\n",
    "    score_df = run_experiment(X=X_train_scaled, Y=y_tr, n_fold=n_fold, alg=alg, alg_params=params[alg], search_type=\"random\", num_searches=num_searches, save_results=save_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display models ranked by CV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = score_df.sort_values(by=\"score\", axis=0)\n",
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from CSV File and re-produce models ranked by CV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.read_csv(save_results)\n",
    "score_df = score_df.sort_values(by=\"score\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve top scoring row\n",
    "best = score_df.iloc[0]\n",
    "display(best)\n",
    "\n",
    "# retrieve model parameters from pandas row\n",
    "alg = best[\"alg\"]\n",
    "params_json = best[\"params_json\"]\n",
    "print(\"alg: {}\".format(alg))\n",
    "print(\"params_json: {}\".format(params_json))\n",
    "\n",
    "# retrieve relevant values\n",
    "alg_cls = alg_map[alg]\n",
    "params = json.loads(params_json)\n",
    "\n",
    "# initialize model\n",
    "model = alg_cls(**params)\n",
    "\n",
    "# train algorithm\n",
    "train_model(X=X_train_scaled, Y=y_tr, n_fold=n_fold, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
